{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2W2zzHm3g8t1"
   },
   "source": [
    "## A Two-stage Feature Selection method for Text Categorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fPc6NtMjg9JY"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WBA8bbPQbg2d",
    "outputId": "eb9c816f-f416-4f7f-c5f6-bdfa11bffa32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "PAjme-WzZ7L4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "Corpus = pd.read_csv('/content/drive/MyDrive/ML_C_Proj/messages.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 189
    },
    "id": "HpAAvQHxh76g",
    "outputId": "d47b6769-fcf7-4f24-9de0-2495001180ac"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>message</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>job posting - apple-iss research center</td>\n",
       "      <td>content - length : 3386 apple-iss research cen...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>lang classification grimes , joseph e . and ba...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>query : letter frequencies for text identifica...</td>\n",
       "      <td>i am posting this inquiry for sergei atamas ( ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>risk</td>\n",
       "      <td>a colleague and i are researching the differin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>request book information</td>\n",
       "      <td>earlier this morning i was on the phone with a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             subject  ... label\n",
       "0            job posting - apple-iss research center  ...     0\n",
       "1                                                NaN  ...     0\n",
       "2  query : letter frequencies for text identifica...  ...     0\n",
       "3                                               risk  ...     0\n",
       "4                           request book information  ...     0\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "jQiKM7Kih7_r"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "#Import Seaborn\n",
    "import seaborn as sns; \n",
    "sns.set()\n",
    "\n",
    "#Import matplot library\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ddFE-11mhyDB",
    "outputId": "2914267a-69db-4f0f-84d0-38eb5297af57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to /root/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 90,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4sWYOmhWbvSq"
   },
   "source": [
    "#Step 1. Remove stop words, punctuation, and non-alphanumeric text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "--r8JjWkU3La"
   },
   "outputs": [],
   "source": [
    "# Step - a : Remove blank rows if any.\n",
    "# Corpus = Corpus[:1000]\n",
    "Corpus['message'].dropna(inplace=True)\n",
    "# Step - b : Change all the text to lower case. This is required as python interprets 'dog' and 'DOG' differently\n",
    "Corpus['message'] = [entry.lower() for entry in Corpus['message']]\n",
    "# Step - c : Tokenization : In this each entry in the corpus will be broken into set of words\n",
    "Corpus['message'] = [word_tokenize(entry) for entry in Corpus['message']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yIdSTWM9VF4B",
    "outputId": "99040a95-7acb-4fb3-9fca-a5ac13c27139"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lang', 'classification', 'grimes', ',', 'joseph', 'e', '.', 'and', 'barbara', 'f', '.', 'grimes', ';', 'ethnologue', 'language', 'family', 'index', ';', 'pb', '.', 'isbn', ':', '0-88312', '-', '708', '-', '3', ';', 'vi', ',', '116', 'pp', '.', ';', '$', '14', '.', '00', '.', 'summer', 'institute', 'of', 'linguistics', '.', 'this', 'companion', 'volume', 'to', 'ethnologue', ':', 'languages', 'of', 'the', 'world', ',', 'twelfth', 'edition', 'lists', 'language', 'families', 'of', 'the', 'world', 'with', 'sub-groups', 'shown', 'in', 'a', 'tree', 'arrangement', 'under', 'the', 'broadest', 'classification', 'of', 'language', 'family', '.', 'the', 'language', 'family', 'index', 'facilitates', 'locating', 'language', 'names', 'in', 'the', 'ethnologue', ',', 'making', 'the', 'data', 'there', 'more', 'accessible', '.', 'internet', ':', 'academic', '.', 'books', '@', 'sil', '.', 'org', 'languages', ',', 'reference', 'lang', '&', 'culture', 'gregerson', ',', 'marilyn', ';', 'ritual', ',', 'belief', ',', 'and', 'kinship', 'in', 'sulawesi', ';', 'pb', '.', ':', 'isbn', ':', '0-88312', '-', '621', '-', '4', ';', 'ix', ',', '194', 'pp', '.', ';', '$', '25', '.', '00', '.', 'summer', 'institute', 'of', 'linguistics', '.', 'seven', 'articles', 'discuss', 'five', 'language', 'groups', 'in', 'sulawesi', ',', 'indonesia', ';', 'the', 'primary', 'focus', 'is', 'on', 'cultural', 'matters', ',', 'with', 'some', 'linguistic', 'content', '.', 'topics', 'include', 'traditional', 'religion', 'and', 'beliefs', ',', 'certain', 'ceremonies', ',', 'and', 'kinship', '.', 'internet', ':', 'academic', '.', 'books', '@', 'sil', '.', 'org', 'language', 'and', 'society', ',', 'indonesia', 'computers', '&', 'ling', 'weber', ',', 'david', 'j', '.', ',', 'stephen', 'r', '.', 'mcconnel', ',', 'diana', 'd', '.', 'weber', ',', 'and', 'beth', 'j', '.', 'bryson', ';', 'primer', ':', 'a', 'tool', 'for', 'developing', 'early', 'reading', 'materials', ';', 'pb', '.', ':', 'isbn', ':', '0-88313', '-', '678', '-', '8', ';', 'xvi', ',', '266', 'pp', '.', '+', 'ms-dos', 'software', ';', '$', '26', '.', '00', '.', 'summer', 'institute', 'of', 'linguistics', '.', 'the', 'authors', 'present', 'a', 'computer', 'program', 'and', 'instructions', 'for', 'developing', 'reading', 'materials', 'in', 'languages', 'with', 'little', 'or', 'no', 'background', 'in', 'literacy', '.', 'the', 'book', 'is', 'structured', 'as', 'a', 'how-to', 'manual', 'with', 'step', 'by', 'step', 'procedures', 'to', 'establish', 'an', 'appropriate', 'primer', 'sequence', 'and', 'to', 'organize', 'words', ',', 'phrases', ',', 'and', 'sentences', 'that', 'correlate', 'with', 'the', 'sequence', '.', 'it', 'presupposes', 'a', 'thorough', 'knowledge', 'of', 'linguistics', '.', 'internet', ':', 'academic', '.', 'books', '@', 'sil', '.', 'org', 'literacy', ',', 'computer']\n"
     ]
    }
   ],
   "source": [
    "print(Corpus['message'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8VfJdeFdhZ1Y"
   },
   "outputs": [],
   "source": [
    "# Step - d : Remove Stop words, Non-Numeric and perfom Word Stemming/Lemmenting.\n",
    "# WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "for index,entry in enumerate(Corpus['message']):\n",
    "    # Declaring Empty List to store the words that follow the rules for this step\n",
    "    Final_words = []\n",
    "    # Initializing WordNetLemmatizer()\n",
    "    word_Lemmatized = WordNetLemmatizer()\n",
    "    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "    for word, tag in pos_tag(entry):\n",
    "        # Below condition is to check for Stop wLoadingords and consider only alphabets\n",
    "        if word not in stopwords.words('english') and word.isalpha():\n",
    "            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "            Final_words.append(word_Final)\n",
    "    # The final processed set of words for each iteration will be stored in 'text_final'\n",
    "    Corpus.loc[index,'text_final0'] = str(Final_words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E-GHuwnMfX2_"
   },
   "outputs": [],
   "source": [
    "print(Corpus['text_final0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eJD4uLD2VF9Q"
   },
   "outputs": [],
   "source": [
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "def clean_sent(sent):\n",
    "    return \" \".join(w for w in nltk.wordpunct_tokenize(sent) \\\n",
    "     if w.lower() in words or not w.isalpha())\n",
    "    \n",
    "Corpus['text_final0'] = Corpus['text_final0'].apply(clean_sent)\n",
    "\n",
    "print(Corpus['text_final0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2WpsJO8Lb0eS"
   },
   "outputs": [],
   "source": [
    "def FCD(Corpus, max_features):\n",
    "  Corpus_sorted = Corpus.sort_values(by=['label'])\n",
    "  print(Corpus_sorted['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99Qo9mjBVGCa"
   },
   "source": [
    "#Splitting data into Training and Testing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "neg_eLMn4ulF"
   },
   "outputs": [],
   "source": [
    "Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(Corpus['text_final0'],Corpus['label'],test_size=0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nhVC6uQ74w6I"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(Test_Y)\n",
    "lb=df.value_counts().index.tolist()\n",
    "val=df.value_counts().values.tolist()\n",
    "exp=(0.025,0)\n",
    "clr=('orange','blue')\n",
    "plt.figure(figsize=(10,5),dpi=140)\n",
    "plt.pie(x=val,explode=exp,labels=lb,colors=clr,autopct='%2.0f%%',pctdistance=0.5, shadow=True,radius=0.9)\n",
    "plt.legend([\"0 = NO SPAM\",'1 = SPAM'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r4t7Gg3RaLcd"
   },
   "outputs": [],
   "source": [
    "# Encoder = LabelEncoder()\n",
    "# Train_Y = Encoder.fit_transform(Train_Y)\n",
    "# Test_Y = Encoder.fit_transform(Test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "abSTi2wqkKZr"
   },
   "outputs": [],
   "source": [
    "print(Corpus.shape)\n",
    "print(Train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2KEs0-hatUi"
   },
   "source": [
    "# Step 2. Calculate the normalized TFIDF in the corresponding element of the weight matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8dJnIbSZaLfc"
   },
   "outputs": [],
   "source": [
    "Tfidf_vect = TfidfVectorizer(max_features=15000)\n",
    "Tfidf_vect.fit(Corpus['text_final0'])\n",
    "Train_X_Tfidf = Tfidf_vect.transform(Train_X)   \n",
    "Test_X_Tfidf = Tfidf_vect.transform(Test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hO3MWUX7kA3F"
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "SVD = sklearn.decomposition.TruncatedSVD(n_components=2, algorithm='randomized', n_iter=5, random_state=None, tol=0.0)\n",
    "SVD.fit(Train_X_Tfidf)\n",
    "Train_X_SVD = SVD.transform(Train_X_Tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BM6EgJuqe4-m"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, stop_words='english', use_idf=True)\n",
    "lsa = TruncatedSVD(n_components=100)\n",
    "Tfidf_vect.fit(Corpus['text_final0'])\n",
    "\n",
    "train_text = vectorizer.fit_transform(Train_X)\n",
    "test_text = vectorizer.fit_transform(Test_X)\n",
    "\n",
    "train_text = lsa.fit_transform(train_text)\n",
    "test_text = lsa.fit_transform(test_text)\n",
    "\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "clf.fit(train_text, Train_Y)\n",
    "clf.score(test_text,Test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HevhCQiRe5aV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Novel_Text_Classifier.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
